{{- $hfApiKey := .Values.hfApiKey | toString }}
{{- $vllmCommand := .Values.vllmCommand | nindent 16}}

{{- $namespace := .Release.Namespace | toString }}

{{- $labels := "" }}
{{- with include "inference-benchmark-backend.labels" . }}
    {{- $labels = . | nindent 4 }}
{{- end }}

{{- $selectorLabels := "" }}
{{- with include "inference-benchmark-backend.labels" . }}
    {{- $selectorLabels = . | nindent 6 }}
{{- end }}

{{- $jobName := "" }}

{{- $appPrefix := include "inference-benchmark-backend.fullname" .  }}
{{- $masterAddr := print $appPrefix "-0." $appPrefix "." $namespace ".svc.cluster.local" }}
{{- $i := "" }}
{{- $rayCommand := "" }}
{{- $start := 0 }}
{{- $nnodes := .Values.MultiprocessingDistributed.replicas | int  }}
{{- range untilStep $start $nnodes 1 }}
  {{- $i = . | toString }}
  {{- $jobName = print $appPrefix "-" $i }}
  {{- if eq . 0 }}
    {{ $rayCommand = "--head --port=6379"}}
  {{- else }}
    {{ $rayCommand = print "--address=" $masterAddr ":6379"}}
  {{- end }}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ $jobName }}
  namespace: {{ $namespace }}
  labels:
    {{- $labels}}
spec:
  selector:
    matchLabels:
      app: {{ $appPrefix }}
  template:
    metadata:
      labels:
        app: {{ $appPrefix }}
    spec:
      subdomain: {{ $appPrefix }}
      hostname: {{ $jobName }}
      containers:
      {{ if eq . 0 }}
        - name: load-generator
          image: ghcr.io/cyril-k/load-generator:latest
          command: ["fastapi", "run", "app/app.py", "--host", "0.0.0.0", "--port", "8080"]
          ports:
            - containerPort: 8080
              protocol: TCP
          resources:
            limits:
              cpu: "8"
              memory: "16Gi"
            requests:
              cpu: "4"
              memory: "8Gi"
          env:
            - name: HF_TOKEN
              value: {{ $hfApiKey }}
      {{ end }}
        - name: {{ $jobName }}
          image: vllm/vllm-openai:v0.6.2
          imagePullPolicy: Always
          securityContext:
            privileged: true
          ports:
            - containerPort: 8000
              protocol: TCP
            - name: workers
              containerPort: 6379
          resources:
            limits:
              cpu: "152" 
              memory: "1200Gi"
              nvidia.com/gpu: 8
            requests:
              cpu: "120"
              memory: "600Gi"
              nvidia.com/gpu: 8
          command:
            - /bin/bash
            - -c
            - |
            {{- if eq . 0 }}
              echo "starting Ray head process..."
              ray start --block {{ $rayCommand }} > /dev/null &
              sleep 30
              echo "done"

              get_node_count() {
                ray status | grep -c "node_"
              }

              while true; do
                current_node_count=$(get_node_count)
                if [[ "$current_node_count" -eq "{{ $nnodes }}" ]]; then
                  {{ $vllmCommand }}
                  break
                fi
                echo "waiting for Ray worker node(s) to be available..."
                sleep 10
              done
            {{- else }}
              ray start --block {{ $rayCommand }}
            {{- end }}
          env:
            - name: DO_NOT_TRACK
              value: "1"
            - name: NCCL_SOCKET_IFNAME
              value: "eth0"
            - name: GLOO_SOCKET_IFNAME
              value: "eth0"
            - name: NCCL_IB_HCA
              value: "mlx5"
            - name: HF_TOKEN
              value: {{ $hfApiKey }}
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /root/.cache/huggingface
              name: cache-volume
            - mountPath: /model-storage
              name: model-storage-volume
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        - name: cache-volume
          emptyDir: { }
        - name: model-storage-volume
          persistentVolumeClaim:
            claimName: csi-s3-model-storage-pvc-v1 
            readOnly: false
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nvidia.com/gpu.count
                    operator: In
                    values:
                      - "1"
                      - "8"
{{- end }}