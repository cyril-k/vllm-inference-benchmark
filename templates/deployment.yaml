apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "inference-benchmark-backend.fullname" . }}
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "inference-benchmark-backend.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "inference-benchmark-backend.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "inference-benchmark-backend.labels" . | nindent 8 }}
        {{- with .Values.podLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      
      # serviceAccountName: {{ include "inference-benchmark-backend.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: load-generator
          image: ghcr.io/cyril-k/load-generator:latest
          command: ["fastapi", "run", "app/app.py", "--host", "0.0.0.0", "--port", "8080"]
          ports:
            - containerPort: 8080
              protocol: TCP
          resources:
            limits:
              cpu: "8"  # Minimal CPU and memory for the sidecar
              memory: "16Gi"
            requests:
              cpu: "4"
              memory: "8Gi"
          env:
            - name: HF_TOKEN
              value: ""
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: vllm/vllm-openai:v0.6.2
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - containerPort: 8000
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 240
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 240
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          startupProbe:
            failureThreshold: 360
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 5
          resources:
            limits:
              cpu: "152" 
              memory: "1200Gi"
              nvidia.com/gpu: 8
            requests:
              cpu: "120"
              memory: "600Gi"
              nvidia.com/gpu: 8
          args:
            # - --model=meta-llama/Llama-3.1-8B-Instruct
            - --model=/model-storage/meta-llama/Llama-3.1-405B-Instruct-FP8
            # - --model=/model-storage/neuralmagic/Meta-Llama-3.1-405B-Instruct-FP8
            - --served-model-name=meta-llama/Llama-3.1-405B-Instruct-FP8
            # - --served-model-name=meta-llama/Llama-3.1-8B-Instruct
            - --tensor-parallel-size=8
            - --max-num-seqs=256
            # - --max-num-seqs=1024
            # - --max-num-batched-tokens=32768
            - --max-num-batched-tokens=16384 
            # - --max-num-batched-tokens=8192
            # <--
            # - --max-num-batched-tokens=128000
            # - --max-num-batched-tokens=65536
            # - --max-model-len=65536
            # - --max-model-len=116000 <--
            - --max-model-len=16384 
            # - --max-model-len=8192
            # <--
            # - --max-model-len=11600
            # - --max-model-len=128000
            # - --gpu-memory-utilization=0.9
            # - --gpu-memory-utilization=0.95
            - --num-scheduler-steps=10
            # - --enable-chunked-prefill
            - --enable-chunked-prefill=False
            - --seed=42
            - --disable-async-output-proc
            - --disable-log-stats
          env:
            - name: DO_NOT_TRACK
              value: "1"
            - name: HF_TOKEN
              value: ""
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /root/.cache/huggingface
              name: cache-volume
            # - name: ib
            #   mountPath: /dev/infiniband
            # - mountPath: /topo-config
            #   name: topo-config-volume
            - mountPath: /model-storage
              name: model-storage-volume
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        - name: cache-volume
          emptyDir: { }
        # - name: ib
        #   hostPath:
        #     path: /dev/infiniband
        # - name: topo-config-volume
        #   configMap:
        #     name: topo-config
        - name: model-storage-volume
          persistentVolumeClaim:
            claimName: csi-s3-model-storage-pvc-v1 
            readOnly: false
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nvidia.com/gpu.count
                    operator: In
                    values:
                      - "1"
                      - "8"
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
