# Default values for inference-benchmark-backend.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 2
# servedModel: meta-llama/Llama-3.1-8B-Instruct
servedModel: meta-llama/Llama-3.1-405B-Instruct-FP8

benchmarkSpecs:
  - dataset_name: sharegpt
    num_prompts: 1000
    request_rate: 6
    backend: openai-chat 
    endpoint: /v1/chat/completions
    dataset_path: app/ShareGPT_V3_unfiltered_cleaned_split.json
    host: localhost
    port: 8000
    model: meta-llama/Llama-3.1-405B-Instruct-FP8
  - dataset_name: sonnet
    num_prompts: 1000
    request_rate: 12
    sonnet_input_len: 512
    sonnet_prefix_len: 128
    sonnet_output_len: 16
    backend: openai-chat 
    endpoint: /v1/chat/completions
    dataset_path: app/sonnet.txt
    host: localhost
    port: 8000
    model: meta-llama/Llama-3.1-405B-Instruct-FP8

image:
  pullPolicy: Always
  vllmURI: vllm/vllm-openai
  vllmTag: v0.6.2
  loadGeneratorURI: ghcr.io/cyril-k/load-generator
  loadGeneratorTag: latest

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

podAnnotations: 
  prometheus.io/scrape: "true"
  # prometheus.io/port: "8080"

podSecurityContext: {}
  # fsGroup: 2000

securityContext:
  privileged: true


service:
  type: ClusterIP
  port: 8080

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

tolerations: []

hfToken: ""